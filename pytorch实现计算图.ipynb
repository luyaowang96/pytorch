{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy 实现梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 设置超参数\n",
    "x = 0 # 初始值\n",
    "lr = 0.1 # 学习率\n",
    "epochs = 10 # 迭代次数\n",
    "\n",
    "## 定义函数y\n",
    "y = lambda x: x**2+2*x+1\n",
    "\n",
    "## 梯度下降算法求解\n",
    "for epoch in range(epochs):\n",
    "    dx = 2*x+2 # d(y)/d(x)\n",
    "    x -= lr * dx #梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8926258176\n",
      "<function <lambda> at 0x000000000650D840>\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 实现梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad None data tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# 定义一个pytorch类型 且可自动求导的的初始值\n",
    "x = torch.Tensor([1])# 定义一个tensor，相当于np.array\n",
    "x = Variable(x,requires_grad=True) # x转变为一个variable，建立计算图的起点;开启requires_grad表示自动计算梯度\n",
    "\n",
    "print('grad',x.grad,'data',x.data) # grad表示x的梯度属性，表明当前累计的梯度；data表示tensor值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad of epoch0: tensor([4.])\n",
      "grad of epoch1: tensor([3.2000])\n",
      "grad of epoch2: tensor([2.5600])\n",
      "grad of epoch3: tensor([2.0480])\n",
      "grad of epoch4: tensor([1.6384])\n",
      "grad of epoch5: tensor([1.3107])\n",
      "grad of epoch6: tensor([1.0486])\n",
      "grad of epoch7: tensor([0.8389])\n",
      "grad of epoch8: tensor([0.6711])\n",
      "grad of epoch9: tensor([0.5369])\n",
      "tensor([-0.7853])\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 设置计算图:建立一个函数y，以x为变量\n",
    "    y = x**2+2*x+1\n",
    "    # Variable 能自动求导==》requires_grad\n",
    "    y.backward() # 对y做反向传导==》自动计算梯度，由于当前变量为1个，所以不需要指定\n",
    "    print('grad of epoch'+str(epoch)+':',x.grad.data)\n",
    "    \n",
    "    x.data -= lr * x.grad.data\n",
    "    # 在 pytorch 中梯度会累积，则每次需要清0\n",
    "    x.grad.data.zero_() #xx_表示对变量做inplace操作；此处将当前梯度清0\n",
    "print(x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy 做回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[2.01161267]\n",
      " [2.97488811]\n",
      " [4.00599999]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 建立简单的数据集\n",
    "f = lambda x:2*(x**2)+3*x+4\n",
    "n=3\n",
    "x_data = np.arange(n).reshape(1,n)\n",
    "y_data = np.array([f(i) for i in x_data])\n",
    "\n",
    "epochs=400\n",
    "lr = 0.1\n",
    "w = np.ones((3,1),dtype='float64')\n",
    "cost=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 计算梯度\n",
    "    x = np.concatenate((x_data**2,x_data))\n",
    "    x = np.concatenate((x,np.ones((1,n)))) ## 3*10\n",
    "    yhat = np.dot(w.T,x) ## 设计模型，用x预测y\n",
    "    loss = np.average((y_data-yhat)**2) # loss函数使用MSE\n",
    "    cost.append(loss)\n",
    "    dw = (-2/y_data.shape[1])*np.dot(x,(y_data-yhat).T)\n",
    "    w -= dw * lr\n",
    "print('w:',w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch 实现回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2) # 设置随机种子\n",
    "\n",
    "# 设置初始变量\n",
    "x_data = Variable(torch.Tensor([[1],[2],[3]]))\n",
    "y_data = Variable(torch.Tensor([[2],[4],[6]]))\n",
    "\n",
    "epochs=20\n",
    "lr = 0.1\n",
    "w = Variable(torch.FloatTensor([0]),requires_grad = True) ## 不要忘记requires_grad\n",
    "cost=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 计算梯度\n",
    "    yhat = x_data * w\n",
    "    loss = torch.mean((yhat - y_data)**2)\n",
    "    cost.append(loss.data.numpy())\n",
    "    loss.backward() #计算偏导\n",
    "    \n",
    "    # 更新参数\n",
    "    w.data -= lr*w.grad.data\n",
    "    w.grad.data.zero_()\n",
    "print(w.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch实现简单的神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 用pytorh求导来进行\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "torch.manual_seed(2)\n",
    "x_data = Variable(torch.Tensor([[1],[2],[3]]))\n",
    "y_data = Variable(torch.Tensor([[2],[4],[6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 定义模型\n",
    "# class构建一个类，通过class Model写一个神经网络类\n",
    "class Model(torch.nn.Module):\n",
    "    # 初始化\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        # super 用来返回Model的父类，在pytorch下定义的类都是继承一个大的父类torch.nn.Module的父类。\n",
    "        # torch.nn.Module中包含了各种工具，一般我们都是写的都是子类，通过父类我们可以很容易书写子类。\n",
    "        self.linear = torch.nn.Linear(1,1,bias=False)\n",
    "        # 建立一个linear类，bias表示偏置项,建立一个AX+b\n",
    "        \n",
    "        # forward 是torch.nn.Module定义好的模板，表示前向传播\n",
    "    def forward(self,x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "model = Model() # 实例化类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "## 定义损失函数\n",
    "criterion = torch.nn.MSELoss(size_average=True) # 平方误差损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 定义优化函数\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01) #利用梯度下降算法优化model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[1.9871]], requires_grad=True)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 优化过程\n",
    "epochs = 30\n",
    "cost=[]\n",
    "for epoch in range(epochs):\n",
    "    # 建立计算图\n",
    "    y_hat = model(x_data) #预测值\n",
    "    loss = criterion(y_hat,y_data)\n",
    "    cost.append(loss.data)\n",
    "    optimizer.zero_grad()# 对模型参数做一个优化，并且将梯度清0\n",
    "    loss.backward()# 计算梯度\n",
    "    \n",
    "    ## 参数更新\n",
    "    optimizer.step()\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
